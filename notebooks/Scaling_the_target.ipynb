{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6b42c5",
   "metadata": {},
   "source": [
    "# Scaling the target\n",
    "\n",
    "### Should you scale the target in regression tasks?\n",
    "\n",
    "It's fairly well known that it's often use, and sometimes essential, to scale (eg normalize or standardize) the features (i.e. `X`) before training a predictive model on them.\n",
    "\n",
    "The question of when, if everm one should _also_ scale the target, `y`, sometimes comes up.\n",
    "\n",
    "I asked ChatGPT for its opinion, and it surprised me a bit because it almost seems to suggest that you should always scale the target:\n",
    "\n",
    "> Scaling the target variable in a regression task can be beneficial in certain scenarios:\n",
    "> \n",
    "> 1. When using regularization techniques: If you are using a regression algorithm with regularization, such as Lasso or Ridge regression, scaling the target variable can help ensure that the magnitude of the coefficients and the regularization penalty term are on a similar scale. This can lead to more stable and accurate models.\n",
    "> \n",
    "> 2. When using neural networks: Neural networks often work better when both input features and target variables are on a similar scale, as it can help with gradient-based optimization and avoid issues like vanishing or exploding gradients. Scaling the target variable in this context can lead to faster convergence and better model performance.\n",
    "> \n",
    "> 3. When comparing models with different target scales: If you are comparing models across different datasets with varying target variable scales, scaling the target variable can help make performance comparisons more consistent and meaningful.\n",
    "> \n",
    "> 4. When using gradient-based optimization algorithms: Some machine learning algorithms, like support vector regression (SVR), use gradient-based optimization methods. Scaling the target variable can help improve the convergence of the optimization process.\n",
    "> \n",
    "> 5. When the target variable has a large range or extreme values: In cases where the target variable has a large range or extreme values, scaling it can help stabilize the learning process and avoid potential issues with numerical stability or model performance.\n",
    "> \n",
    "> However, it is important to remember that after training the model, you will need to reverse the scaling transformation to interpret the model's predictions in their original scale.\n",
    "> \n",
    "> In summary, while not always necessary, scaling the target variable in a regression task can be beneficial in specific situations, such as when using regularization, neural networks, or gradient-based optimization algorithms.\n",
    "\n",
    "And there's [this conversation](https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re) on StackOverflow, with answers ranging from \"No\" to \"Maybe\" to \"Yes\".\n",
    "\n",
    "So let's load some data and try some things!\n",
    "\n",
    "### TLDR\n",
    "\n",
    "Experimentation suggests that very large or very small target values make SVR and MLPRegressor very unstable and hard to train, even with hyperparameter search. (I probably didn't think hard enough about how to compensate for the value magnitudes, mostly just playing with a couple of arguments). In each case, scaling the target (standardizing) solved the problem.\n",
    "\n",
    "As far as I can tell, scaling the target did not substantially affect the performance of LinearRegression (with or without L2 regularization, see below re L1), KNN regression, SGD regression (surprised me), or RandomForestRegressor.\n",
    "\n",
    "For whatever reason, I cannot get Lasso (L1 regularization) to converge at all on my data, even when scaling the input and the output.\n",
    "\n",
    "If you're getting unstable results from an SVR or MLP, I think the best strategy may be to just use another algorithm, rather than trying to find the right hyperparameters, or deal with the hassle of scaling the target. If hellbent on using a neural net or SVR, just scale the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295a42a",
   "metadata": {},
   "source": [
    "## Load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://geocomp.s3.amazonaws.com/data/MD-GR-NPOR-RHOB-DT4P-DT4S.txt',\n",
    "                 names='MD-GR-NPOR-RHOB-DT4P-DT4S'.split('-'),\n",
    "                )\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problem samples.\n",
    "df = df[df['DT4S'] > 200]\n",
    "\n",
    "# Make new targets.\n",
    "df['DT4S_scaled'] = (df['DT4S'] - df['DT4S'].mean()) / df['DT4S'].std()\n",
    "df['DT4S_huge'] = df['DT4S'] * 1e6\n",
    "df['DT4S_tiny'] = df['DT4S'] / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804296bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Make X and y.\n",
    "X = df[['MD', 'GR', 'NPOR', 'RHOB', 'DT4P']].values\n",
    "y = df['DT4S_tiny'].values\n",
    "\n",
    "# Split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0a31c",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Should not make a difference if I scale the target or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and fit pipeline.\n",
    "regr = LinearRegression()\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909653f3",
   "metadata": {},
   "source": [
    "With scaled target (the [`TransformedTargetRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) class helps a lot because transforming the target is super-fiddly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(LinearRegression(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e4918",
   "metadata": {},
   "source": [
    "Now with regularization, using `Ridge()`. The idea is that if you use regularization then it might be a good idea to scale the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720397dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = Ridge()\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339509ed",
   "metadata": {},
   "source": [
    "With scaled target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555606fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(Ridge(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a095ed",
   "metadata": {},
   "source": [
    "Makes no difference in either case, i.e. regularization or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856402f9",
   "metadata": {},
   "source": [
    "Now with `Lasso()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872327f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "regr = Lasso(alpha=0)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3955be9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "regr = Lasso()\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(Lasso(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0e018",
   "metadata": {},
   "source": [
    "`Lasso` does not converge at all and I don't know why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba26a60",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc49d83",
   "metadata": {},
   "source": [
    "Now with `RandomForestRegressor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736788f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor()\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(RandomForestRegressor(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc829c35",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe90c6c",
   "metadata": {},
   "source": [
    "And with SGD -- tiny not affect, huge is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42971ceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = SGDRegressor(penalty=None)  # No regularization.\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9d3c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = SGDRegressor(alpha=0.01)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(SGDRegressor(alpha=0.01), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34c5d4",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c00c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "regr = MLPRegressor(alpha=0,  max_iter=1000)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b282cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = MLPRegressor(alpha=0.0001, max_iter=1000)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(MLPRegressor(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711233a7",
   "metadata": {},
   "source": [
    "ðŸ’¥ So this one does blow up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3849134",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89038b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "regr = KNeighborsRegressor(n_neighbors=50)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8e2aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = KNeighborsRegressor()\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(KNeighborsRegressor(), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f4100",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b326e64",
   "metadata": {},
   "source": [
    "And with SVR -- all affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08e0fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = SVR(C=1e12)  # Almost no regularization.\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a68873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = SVR(C=1)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(SVR(C=1), transformer=StandardScaler())\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad9d3d",
   "metadata": {},
   "source": [
    "## Question: can you always rescue things with (say) GridSearch\n",
    "\n",
    "**Short answer: Maybe, if you know which params to change and really check all the cases (esp near edges!). But for SVR at least, tiny values seem to be very difficult to compensate for.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ec64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SVR(C=1)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a81294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid={'svr__epsilon': np.logspace(-6, 6, 13)}, cv=6, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5675260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid={'svr__C': np.logspace(-6, 6, 13),\n",
    "                                      'svr__epsilon': np.logspace(-6, 6, 13)},\n",
    "                    cv=6, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4aea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eced1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadda0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SVR(C=1e6)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SVR(C=1e9)\n",
    "pipe = make_pipeline(StandardScaler(), regr)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3e477",
   "metadata": {},
   "source": [
    "Conclusion: probably better to just use another algorithm rather than trying to resuce stability from SVR or MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5e39a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "&copy; 2023 Matt Hall, licensed CC BY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redflag",
   "language": "python",
   "name": "redflag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
