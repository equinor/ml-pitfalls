{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mlutils import decision_regions\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mis-steps in machine learning\n",
    "\n",
    "First we'll import some data. I'm using an extract from the Rock Property Catalog, https://subsurfwiki.org/wiki/Rock_Property_Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://geocomp.s3.amazonaws.com/data/RPC_simple.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to predict lithology from `Vp` and `rho`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's usually easier to look at a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ What do we think of this dataset?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear model: SVM\n",
    "\n",
    "The **support vector machine** or SVM is a good model to start supervised classification with. It attempts to separate the classes with lines.\n",
    "\n",
    "We will make a prediction called `y_pred`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [1400, 6500, 1.9, 2.9]\n",
    "y_pred, y_all = decision_regions(svc, X, y, extent, step=(2, 0.005))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(y_all <= 0.0, extent=extent, origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X.T, c=is_sand(y), s=80, cmap='bwr')\n",
    "plt.scatter(*X.T, c=is_sand(y_pred))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'd like an accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>❓ What do we think of this?</h2>\n",
    "\n",
    "- What is 'good performance'?\n",
    "- What could make a good benchmark?\n",
    "- What would make a good lower bound? (Check out [the imbalance notebook](Balance_classes_with_SMOTE.ipynb).)\n",
    "- Is there an upper bound?\n",
    "\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Validation\n",
    "\n",
    "We only tested the model performance against the training data. We need to check against some rocks the model has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ What could we do about it? What rocks can we use? How many do we need?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>\n",
    "\n",
    "Split off the first 300 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X[:300], X[300:]\n",
    "y_train, y_val = y[:300], y[300:]\n",
    "\n",
    "plt.scatter(*X_train.T, label='train')  # Blue points.\n",
    "plt.scatter(*X_val.T, label='val')      # Orange points.\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>\n",
    "\n",
    "That's no good. We need a random split instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_train.T, label='train')\n",
    "plt.scatter(*X_val.T, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Can you think of some occasions when a random split might not be okay?\n",
    "\n",
    "- Certain kinds of data?\n",
    "- Certain proportions of classes?\n",
    "- Do we really want to roll the dice every time?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_train.T, label='train')\n",
    "plt.scatter(*X_val.T, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Will the model be better or worse now?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>\n",
    "\n",
    "Because we're using less data to train, and because we're no longer checking the performance against data we trained on, the model probably gets a bit less predictive... but maybe we trust the prediction of future accuracy more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we care about the score against the training data?\n",
    "\n",
    "Would we expect it to be lower or higher than the validation data?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = svc.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_train, y_pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_all = decision_regions(svc, X_val, y_val, extent, step=(2, 0.005))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(y_all <= 0.0, extent=extent, origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X_train.T, c=is_sand(y_train), marker='+', cmap='bwr', alpha=0.50)\n",
    "plt.scatter(*X_val.T, c=is_sand(y_val), s=80, cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A more complex model\n",
    "\n",
    "Notice that the model is linear. This makes the possibly big assumption that the decision boundary is linear in the feature space.\n",
    "\n",
    "\n",
    "### ❓ Will a non-linear model do better or worse?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>\n",
    "\n",
    "Let's try the default non-linear SVM, which uses a 'radial basis function' kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_all = decision_regions(svc, X_val, y_val, extent, step=(2, 0.005))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(y_all <= 0.0, extent=extent, origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X_train.T, c=is_sand(y_train), marker='+', cmap='bwr', alpha=0.50)\n",
    "plt.scatter(*X_val.T, c=is_sand(y_val), s=80, cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ The model is terrible. Why?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X.T, c=is_sand(y), s=20*(1+is_sand(y)), cmap='bwr')\n",
    "plt.axis('equal')  # <-- So we can see the data space as sklearn 'sees' it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_all = decision_regions(svc, X_val, y_val, [-500, 8000, -2500, 2500], step=(20, 20))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(y_all <= 0.0, extent=[-500, 8000, -2500, 2500], origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X_train.T, c=is_sand(y_train), s=20*(1+is_sand(y_train)), cmap='bwr')\n",
    "plt.axis('equal')  # <-- So we can see the data space as sklearn 'sees' it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ What can we do about it?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Standardization\n",
    "\n",
    "We gave the model our raw data. We get away with it with the linear model, but any algorithms that depend on distance — either in the learning algorithm, or the cost function, or in regularization — would prefer to have standardized data. That way, they work in 'Z-score' space. (When you plot with `matplotlib` it's doing a min/max scaling on both axes so that the points look reasonable. It's a similar idea.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Now what do I do?\n",
    "\n",
    "<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_train.T, label='train')\n",
    "plt.scatter(*X_val.T, label='val')\n",
    "plt.legend()\n",
    "plt.axis('equal')  # <-- So we can see the data space as sklearn 'sees' it.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model should do about as well as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the non-linear model will be much better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [-3, 3, -3, 3]\n",
    "\n",
    "y_pred, y_all = decision_regions(svc, X_val, y_val, extent, step=(0.01, 0.01))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_all <= 0.0, extent=extent, origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X_train.T, c=is_sand(y_train), marker='+', cmap='bwr', alpha=0.5)\n",
    "plt.scatter(*X_val.T, c=is_sand(y_val), s=80, cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_pred))\n",
    "plt.axis('equal')\n",
    "plt.xlim(-2.5, 2.5); plt.ylim(-2.5, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice at the bottom that the model is still not suitable for extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Untuned model\n",
    "\n",
    "We didn't try to adjust hyperparameters to get a better fit. Turns out, if you do this, the model does better with a different value for `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for C in (Cs := np.logspace(-3, 4, 15)):\n",
    "    svc = SVC(C=C).fit(X_train, y_train)\n",
    "    vals.append(svc.score(X_val, y_val))\n",
    "    \n",
    "plt.plot(Cs, vals, 'o-')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0.9, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=100)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "\n",
    "plt.scatter(*X_val.T, c=is_sand(y_val), s=80, cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_all = decision_regions(svc, X_val, y_val, extent, step=0.02)\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_all <= 0.0, extent=extent, origin='lower', aspect='auto', alpha=0.5, interpolation='none')\n",
    "plt.scatter(*X_train.T, c=is_sand(y_train), marker='+', cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_val), s=80, cmap='bwr')\n",
    "plt.scatter(*X_val.T, c=is_sand(y_pred))\n",
    "plt.axis('equal')\n",
    "plt.xlim(-2.5, 2.5); plt.ylim(-2.5, 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to make a decision about what we think is more reasonable.\n",
    "\n",
    "After that, there are plenty more gotchas:\n",
    "\n",
    "- We have assumed that the labels are correct and the data is accurate.\n",
    "- A few hundred records is not much data; we should be careful about where we apply this model.\n",
    "- We have only tried one model type, and have not tuned all of its hyperparameters.\n",
    "- As we add more features, we have to remember the curse of dimensionality.\n",
    "- As we try more things, we need to start using a `test` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "&copy; 2023 Matt Hall, licensed CC BY"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "886c583f8a98883be098d5088d8db03d22bd092a36a57e3c48aaf8e50654a668"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
